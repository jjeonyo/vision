import os
from dotenv import load_dotenv

# 1) LLM & Embedding (ê·¸ëŒ€ë¡œ OK)
from langchain_google_genai import (
    ChatGoogleGenerativeAI,
    GoogleGenerativeAIEmbeddings,
)

# 2) VectorStore (ê·¸ëŒ€ë¡œ OK)
from langchain_community.vectorstores import SupabaseVectorStore

# 3) ì˜ˆì „ memory, chains â†’ classicìœ¼ë¡œ ì´ë™
from langchain_classic.memory import ConversationBufferMemory
from langchain_classic.chains import ConversationalRetrievalChain

# 4) PromptTemplateëŠ” coreë¡œ ì´ë™
from langchain_core.prompts import PromptTemplate

# 5) Supabase í´ë¼ì´ì–¸íŠ¸
from supabase import create_client, Client

from langchain_community.vectorstores import SupabaseVectorStore
from langchain_core.documents import Document

# ë§¨ ìœ„ import ê·¼ì²˜ì— ì¶”ê°€
from langchain_core.documents import Document
from langchain_community.vectorstores import SupabaseVectorStore


def _patched_similarity_search_by_vector_with_relevance_scores(
    self,
    query,
    k,
    filter=None,           # ì§€ê¸ˆì€ Supabase í•¨ìˆ˜ì— filter ì¸ì ì—†ìœ¼ë‹ˆ ì•ˆ ì”€
    postgrest_filter=None, # ì´ê²ƒë„ ë¬´ì‹œ
    score_threshold=None,
):
    # âš  ì—¬ê¸°ê°€ Supabase RPCë¡œ ë‚ ì•„ê°€ëŠ” íŒŒë¼ë¯¸í„°
    # ì—ëŸ¬ì—ì„œ íŒíŠ¸ ì¤€ ê·¸ëŒ€ë¡œ:
    #   match_manual_sections(match_count, match_threshold, query_embedding)
    filter_model_id = None
    if isinstance(filter, dict):
        filter_model_id = filter.get("model_id")

    match_documents_params = {
        "match_count": k,
        "match_threshold": 0.7,   # ìœ ì‚¬ë„ í•˜í•œ(ì˜ˆì‹œ). ì›í•˜ë©´ 0.6~0.8 ì‚¬ì´ë¡œ ì¡°ì •
        "query_embedding": query,
        "filter_model_id": filter_model_id,
    }

    # Supabase RPC í˜¸ì¶œ
    response = self._client.rpc(self.query_name, match_documents_params).execute()
    rows = response.data or []

    docs_and_scores = []

    for row in rows:
        content = row.get("content", "")
        if not content:
            continue

        metadata = row.get("metadata") or {}
        score = float(row.get("similarity", 0.0))

        # LangChain ìª½ì—ì„œ score_threshold ë” ì£¼ë©´ ì—¬ê¸°ì„œ í•œ ë²ˆ ë” í•„í„°ë§
        if score_threshold is not None and score < score_threshold:
            continue

        doc = Document(page_content=content, metadata=metadata)
        docs_and_scores.append((doc, score))

    # í˜¹ì‹œ Supabase í•¨ìˆ˜ê°€ match_countë³´ë‹¤ ë§ì´ ëŒë ¤ì¤˜ë„ ìƒìœ„ kê°œë§Œ ì‚¬ìš©
    if k is not None and len(docs_and_scores) > k:
        docs_and_scores = docs_and_scores[:k]

    return docs_and_scores


# SupabaseVectorStoreì— íŒ¨ì¹˜ ì ìš©
SupabaseVectorStore.similarity_search_by_vector_with_relevance_scores = (
    _patched_similarity_search_by_vector_with_relevance_scores
)



# 1. í™˜ê²½ë³€ìˆ˜ ë° í‚¤ ì„¤ì •
load_dotenv() # .env íŒŒì¼ì´ ìˆë‹¤ë©´ ë¡œë”©
SUPABASE_URL = "https://wzafalbctqkylhyzlfej.supabase.co"
SUPABASE_KEY = os.getenv("supbase_service_role")
GOOGLE_API_KEY = os.getenv("google_api")

# 2. Supabase & Gemini ì„¤ì •
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# ì„ë² ë”© ëª¨ë¸ (ëˆˆ)
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/text-embedding-004",
    google_api_key=GOOGLE_API_KEY,
    task_type="retrieval_query"
)

# LLM ëª¨ë¸ (ë‘ë‡Œ) - Gemini 2.5 Flash
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=GOOGLE_API_KEY,
    temperature=0.1 # ë§¤ë‰´ì–¼ ë‹µë³€ì´ë¯€ë¡œ ì°½ì˜ì„± ë‚®ì¶¤
)

# 3. Vector Store ì—°ê²° (ë­ì²´ì¸ì´ ìš°ë¦¬ DBë¥¼ ì“¸ ìˆ˜ ìˆê²Œ í•¨)
vector_store = SupabaseVectorStore(
    client=supabase,
    embedding=embeddings,
    table_name="manual_sections",
    query_name="match_manual_sections" # ì•„ê¹Œ ìˆ˜ì •í•œ RPC í•¨ìˆ˜ ì´ë¦„
)

# ê²€ìƒ‰ê¸°(Retriever) ì„¤ì •
# -> k=3: 3ê°œë§Œ ì°¾ì•„ì™€ë¼
# -> filter: F24WD ëª¨ë¸ ê²ƒë§Œ ì°¾ì•„ë¼ (ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê°€ëŠ¥!)
retriever = vector_store.as_retriever(
    search_kwargs={"k": 3} 
    # í•„ìš”í•˜ë©´ ì—¬ê¸°ì— {'filter': {'model_id': '...'}} ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
)

# 4. ê¸°ì–µ(Memory) ì¥ì¹˜ ì¶”ê°€
# -> ëŒ€í™” ë‚´ìš©ì„ 'chat_history'ë¼ëŠ” í‚¤ì— ì €ì¥í•´ì„œ ê³„ì† ë“¤ê³  ë‹¤ë‹˜
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

# 5. í”„ë¡¬í”„íŠ¸ (í˜ë¥´ì†Œë‚˜) ì„¤ì •
custom_template = """
ë‹¹ì‹ ì€ LGì „ì ê°€ì „ì œí’ˆ ì „ë¬¸ ìƒë‹´ì› 'ThinQ ë´‡'ì…ë‹ˆë‹¤.
ì•„ë˜ì˜ [ë¬¸ë§¥(Context)]ê³¼ [ëŒ€í™” ê¸°ë¡(Chat History)]ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µí•˜ì„¸ìš”.
ë§Œì•½ ë§¤ë‰´ì–¼ì— ì—†ëŠ” ë‚´ìš©ì´ë©´ ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.

1. í‘œ ë‚´ìš©ì€ ë¬¸ì¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
2. í•­ìƒ ì •ì¤‘í•œ ë§íˆ¬(í•˜ì‹­ì‹œì˜¤ì²´ ë˜ëŠ” í•´ìš”ì²´)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.


[ëŒ€í™” ê¸°ë¡]:
{chat_history}

[ë¬¸ë§¥(ë§¤ë‰´ì–¼ ê²€ìƒ‰ ê²°ê³¼)]:
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]: {question}

[ë‹µë³€]:
"""

QA_PROMPT = PromptTemplate(
    template=custom_template,
    input_variables=["chat_history", "context", "question"]
)

# 6. ì²´ì¸ ìƒì„± (ë‘ë‡Œ + ê¸°ì–µ + ê²€ìƒ‰ê¸° ì—°ê²°)
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=True, # ë‹µë³€ ì¶œì²˜ë„ ê°™ì´ ë°˜í™˜
    combine_docs_chain_kwargs={"prompt": QA_PROMPT} # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì ìš©
)

def main():
    print("ğŸ¤– LG ThinQ ì±—ë´‡ (LangChain ë²„ì „) - ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥")
    print("-" * 50)
    
    while True:
        query = input("\në‚˜: ")
        if query.lower() == "exit":
            break
            
        # ë­ì²´ì¸ì—ê²Œ ì§ˆë¬¸ ë˜ì§€ê¸° (ìë™ìœ¼ë¡œ ê²€ìƒ‰í•˜ê³ , ê¸°ì–µí•´ì„œ ë‹µë³€í•¨)
        result = qa_chain.invoke({"question": query})
        
        print(f"\nğŸ¤– ë´‡: {result['answer']}")
        
        # (ì„ íƒ) ì¶œì²˜ í™•ì¸
        # print("\n[ì°¸ê³ í•œ ë§¤ë‰´ì–¼]:")
        # for doc in result['source_documents']:
        #     print(f"- {doc.metadata.get('section_title', 'ì œëª©ì—†ìŒ')}")

if __name__ == "__main__":
    main()
