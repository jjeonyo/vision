import os
import time
import google.generativeai as genai
from supabase import create_client, Client
import pdfplumber
from dotenv import load_dotenv
import re
import pytesseract
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
try:
    import pytesseract
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    print("âš ï¸ pytesseractê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ ì´ë¯¸ì§€ OCRì„ ê±´ë„ˆëœë‹ˆë‹¤.")

# ==========================================
# 1. ì„¤ì • ì •ë³´ (ìƒˆ í‚¤ í™•ì¸ í•„ìˆ˜!)
# ==========================================

load_dotenv()  # load variables from .env into environment
SUPABASE_URL = "https://wzafalbctqkylhyzlfej.supabase.co"
SUPABASE_KEY = os.getenv("supbase_service_role")
GOOGLE_API_KEY = os.getenv("google_api")

PDF_FILE_PATH = "MFL67658585.pdf" 
TARGET_DOC_TITLE = "ë“œëŸ¼ ì„¸íƒê¸° ìƒì„¸ ë§¤ë‰´ì–¼ (Table Optimized)"
# ==========================================

genai.configure(api_key=GOOGLE_API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def clean_cell(cell):
    return str(cell).replace('\n', ' ').strip() if cell else ""

def get_embedding(text):
    try:
        time.sleep(2) # ë¬´ë£Œ API ì œí•œ ë³´í˜¸
        result = genai.embed_content(
            model="models/text-embedding-004",
            content=text,
            task_type="retrieval_document"
        )
        return result['embedding']
    except Exception as e:
        print(f"  âš ï¸ ì„ë² ë”© ì—ëŸ¬ (10ì´ˆ ëŒ€ê¸°): {e}")
        time.sleep(10)
        return None

def format_table_row(row, headers=None):
    """
    í‘œì˜ í•œ ì¤„(Row)ì„ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - headersê°€ ìˆìœ¼ë©´ "í—¤ë”: ê°’"ìœ¼ë¡œ ë§¤í•‘í•´ ì €ì¥í•©ë‹ˆë‹¤.
    - headersê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ 3ì—´(ë¬¸ì œìƒí™©/ì›ì¸/í•´ê²°ë°©ë²•) ê·œì¹™ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    cleaned_row = [clean_cell(cell) for cell in row]
    
    # ë‚´ìš©ì´ ë„ˆë¬´ ì ìœ¼ë©´(ë¹ˆ ì¤„) ê±´ë„ˆëœ€
    if all(len(c) < 1 for c in cleaned_row):
        return None

    # í—¤ë”ê°€ ìˆìœ¼ë©´ í—¤ë”:ê°’ í˜•íƒœë¡œ ë³€í™˜
    if headers and len(headers) == len(cleaned_row):
        pairs = []
        for h, val in zip(headers, cleaned_row):
            if not h:
                continue
            pairs.append(f"{h}: {val}" if val else f"{h}: ")
        if pairs:
            return " | ".join(pairs)

    # í—¤ë”ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ 3ì—´ ê·œì¹™
    if len(cleaned_row) >= 3:
        return f"ë¬¸ì œìƒí™©: {cleaned_row[0]} | ì›ì¸: {cleaned_row[1]} | í•´ê²°ë°©ë²•: {cleaned_row[2]}"

    # ì—´ ê°œìˆ˜ê°€ ë¶ˆê·œì¹™í•˜ë©´ ê·¸ëƒ¥ ì´ì–´ ë¶™ì„
    return " | ".join(cleaned_row)

def sanitize_text(text: str) -> str:
    """
    í•œê¸€/ì˜ë¬¸/ìˆ«ì/ê³µë°±ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°í•©ë‹ˆë‹¤.
    """
    cleaned = re.sub(r"[^0-9A-Za-zê°€-í£\s]", " ", text)
    return re.sub(r"\s+", " ", cleaned).strip()

def ocr_images_on_page(page, languages="kor+eng"):
    """
    í˜ì´ì§€ ë‚´ ì´ë¯¸ì§€ ì˜ì—­ì„ OCRí•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    pytesseract ë¯¸ì„¤ì¹˜ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not OCR_AVAILABLE:
        return []

    texts = []
    images = getattr(page, "images", None) or []
    for img in images:
        x0 = img.get("x0")
        x1 = img.get("x1")
        top = img.get("top", img.get("y0"))
        bottom = img.get("bottom", img.get("y1"))
        if None in (x0, x1, top, bottom):
            continue
        try:
            subpage = page.within_bbox((x0, top, x1, bottom))
            pil_img = subpage.to_image(resolution=300).original
            raw_text = pytesseract.image_to_string(pil_img, lang=languages)
            cleaned = sanitize_text(raw_text)
            if len(cleaned) >= 1:
                texts.append(cleaned)
        except Exception:
            continue
    return texts

def upload_manual_to_supabase():
    print(f"ğŸ“‚ [Table Optimized] ì—…ë¡œë“œ ì‹œì‘: {PDF_FILE_PATH}")
    
    # 1. ë¬¸ì„œ ë“±ë¡
    doc_res = supabase.table("manual_documents").insert({
        "title": TARGET_DOC_TITLE,
        "version": "v3.0_table",
        "file_url": "local"
    }).execute()
    doc_id = doc_res.data[0]['doc_id']
    print(f"âœ… ë¬¸ì„œ ID ë°œê¸‰: {doc_id}")

    total_chunks = 0
    
    with pdfplumber.open(PDF_FILE_PATH) as pdf:
        for i, page in enumerate(pdf.pages):
            print(f"ğŸ“– {i+1}í˜ì´ì§€ ë¶„ì„ ì¤‘...")
            
            # ---------------------------------------------------------
            # ì „ëµ A: í‘œ(Table)ê°€ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸í•˜ê³  ì¶”ì¶œ
            # ---------------------------------------------------------
            tables = page.extract_tables()
            
            if tables:
                print(f"  âœ¨ í‘œ {len(tables)}ê°œ ë°œê²¬! í‘œ ëª¨ë“œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
                for table in tables:
                    if not table:
                        continue

                    header_row = [clean_cell(cell) for cell in table[0]] if table else []
                    has_header = any(header_row)
                    body_rows = table[1:] if has_header and len(table) > 1 else table

                    for row in body_rows:
                        # í‘œì˜ í•œ ì¤„ì„ "ë¬¸ì¥"ìœ¼ë¡œ ë§Œë“¦
                        sentence = format_table_row(row, headers=header_row if has_header else None)
                        if not sentence: continue
                        sentence = sanitize_text(sentence)
                        
                        # ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìœ¼ë©´(í—¤ë” ë“±) ìŠ¤í‚µí•˜ê±°ë‚˜ ì €ì¥
                        if len(sentence) < 10: continue

                        # ğŸŒŸ ì´ ë¬¸ì¥ì„ ë²¡í„°í™”í•´ì„œ ì €ì¥ (ì´ê²Œ í•µì‹¬!)
                        vector = get_embedding(sentence)
                        if vector:
                            data = {
                                "doc_id": doc_id,
                                "category": "troubleshooting_table", # ì¹´í…Œê³ ë¦¬ êµ¬ë¶„
                                "section_title": f"{i+1}í˜ì´ì§€ (ê³ ì¥ì¡°ì¹˜ í‘œ)",
                                "content_text": sentence, # "ì¦ìƒ:.. ì›ì¸:.. í•´ê²°:.." í˜•íƒœë¡œ ì €ì¥ë¨
                                "page_number": i + 1,
                                "embedding_vector": vector
                            }
                            supabase.table("manual_sections").insert(data).execute()
                            print(f"    -> [í‘œ ë°ì´í„°] ì €ì¥: {sentence[:30]}...")
                            total_chunks += 1
            
            # ---------------------------------------------------------
            # ì´ë¯¸ì§€ OCR: í‘œ/í…ìŠ¤íŠ¸ ì™¸ ì´ë¯¸ì§€ì— í¬í•¨ëœ ê¸€ìë„ ì¶”ì¶œ
            # ---------------------------------------------------------
            ocr_texts = ocr_images_on_page(page)
            for idx, ocr_text in enumerate(ocr_texts):
                vector = get_embedding(ocr_text)
                if vector:
                    data = {
                        "doc_id": doc_id,
                        "category": "ocr_image",
                        "section_title": f"{i+1}í˜ì´ì§€-OCR{idx+1}",
                        "content_text": ocr_text,
                        "page_number": i + 1,
                        "embedding_vector": vector
                    }
                    supabase.table("manual_sections").insert(data).execute()
                    total_chunks += 1
                
                # í‘œê°€ ìˆëŠ” í˜ì´ì§€ëŠ” í…ìŠ¤íŠ¸ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì—¬ê¸°ì„œ ëë‚¼ ìˆ˜ë„ ìˆì§€ë§Œ,
                # í‘œ ì™¸ì— ë‹¤ë¥¸ ì„¤ëª…ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ì•„ë˜ í…ìŠ¤íŠ¸ ì¶”ì¶œë„ ì§„í–‰í•©ë‹ˆë‹¤.
            
            # ---------------------------------------------------------
            # ì „ëµ B: ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í‘œê°€ ì•„ë‹ˆê±°ë‚˜, í‘œ ë°–ì˜ ë‚´ìš©)
            # ---------------------------------------------------------
            text = page.extract_text()
            if text:
                # í‘œ ë‚´ìš©ì€ ì´ë¯¸ ìœ„ì—ì„œ ì €ì¥í–ˆìœ¼ë‹ˆ, ì¤‘ë³µì„ í”¼í•˜ê¸° ìœ„í•´
                # í…ìŠ¤íŠ¸ê°€ ì•„ì£¼ ê¸¸ ë•Œë§Œ(í‘œ ë§ê³  ë‹¤ë¥¸ ê¸´ ì„¤ëª…ì´ ìˆì„ ë•Œë§Œ) ì €ì¥
                clean_text = text.replace('\n', ' ').strip()
                
                # í‘œë§Œ ìˆëŠ” í˜ì´ì§€ë©´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ìŠ¤í‚µ (ì¤‘ë³µ ë°©ì§€ ê¼¼ìˆ˜)
                if tables and len(clean_text) < 500:
                    continue

                # ì²­í‚¹ ë° ì €ì¥ (ê¸°ì¡´ ë¡œì§)
                chunk_size = 600
                chunks = [clean_text[k:k+chunk_size] for k in range(0, len(clean_text), 500)]
                
                for idx, chunk in enumerate(chunks):
                    chunk = sanitize_text(chunk)
                    if len(chunk) < 50: continue # ë„ˆë¬´ ì§§ìœ¼ë©´ íŒ¨ìŠ¤
                    
                    # 2ì´ˆ ëŒ€ê¸°
                    time.sleep(2)
                    vector = get_embedding(chunk)
                    if vector:
                        data = {
                            "doc_id": doc_id,
                            "category": "general_text",
                            "section_title": f"{i+1}í˜ì´ì§€-ë³¸ë¬¸{idx+1}",
                            "content_text": chunk,
                            "page_number": i + 1,
                            "embedding_vector": vector
                        }
                        try:
                            supabase.table("manual_sections").insert(data).execute()
                            total_chunks += 1
                        except:
                            pass

    print(f"\nğŸ‰ ì™„ë£Œ! ì´ {total_chunks}ê°œì˜ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    upload_manual_to_supabase()
