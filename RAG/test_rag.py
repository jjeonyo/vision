import os
import google.generativeai as genai
from supabase import create_client, Client
from dotenv import load_dotenv

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 2. ì„¤ì • ì •ë³´
SUPABASE_URL = "https://wzafalbctqkylhyzlfej.supabase.co"
SUPABASE_KEY = os.getenv("supbase_service_role")
GOOGLE_API_KEY = os.getenv("google_api")

# 3. í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
genai.configure(api_key=GOOGLE_API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
generation_model = genai.GenerativeModel('gemini-2.5-flash')

def get_embedding(text):
    result = genai.embed_content(
        model="models/text-embedding-004",
        content=text,
        task_type="retrieval_query"
    )
    return result['embedding']

def search_manual(query_text):
    # 1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
    query_vector = get_embedding(query_text)
    
    # 2. Supabase RPC í˜¸ì¶œ (í•˜ì´ë¸Œë¦¬ë“œ í•¨ìˆ˜ ì‚¬ìš©)
    # query_text(í‚¤ì›Œë“œìš©)ì™€ query_embedding(ë²¡í„°ìš©)ì„ ëª¨ë‘ ë³´ëƒ…ë‹ˆë‹¤.
    response = supabase.rpc("hybrid_search", {
        "query_text": query_text,       # í…ìŠ¤íŠ¸ ë§¤ì¹­ìš©
        "query_embedding": query_vector,# ì˜ë¯¸ ê²€ìƒ‰ìš©
        "match_threshold": 0.1,         # ì •í™•ë„ ê¸°ì¤€ (ì¡°ê¸ˆ ë†’ì„)
        "match_count": 5,
        "w_vector": 0.9,                # ë²¡í„° ë¹„ì¤‘ 70%
        "w_keyword": 0.1                # í‚¤ì›Œë“œ ë¹„ì¤‘ 30%
    }).execute()
    
    return response.data

def generate_answer(query_text, context_list):
    if not context_list:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë§¤ë‰´ì–¼ì—ì„œ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ê²€ìƒ‰ëœ ë‚´ìš© ì¡°í•©
    # SQLì—ì„œ 'content_text'ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ë¦¬í„´í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    context_text = "\n\n".join([
        f"- {item.get('content_text', '')} (ì¶œì²˜: {item.get('section_title', 'ì œëª©ì—†ìŒ')})" 
        for item in context_list
    ])

    prompt = f"""
    ë‹¹ì‹ ì€ LGì „ì ê°€ì „ì œí’ˆ ì „ë¬¸ ìƒë‹´ì› 'ThinQ ë´‡'ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì œê³µëœ [ë©”ë‰´ì–¼ ë°ì´í„°]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
    
    1. í‘œ ë‚´ìš©ì€ ë¬¸ì¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
    2. ë‹µë³€ ëì—ëŠ” ì°¸ê³ í•œ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
    3. ì‚¬ìš©ìê°€ 'í†µëŒì´', 'ë“œëŸ¼' ë“± êµ¬ì–´ì²´ë‚˜ íŠ¹ì • ìš©ì–´ë¥¼ ì‚¬ìš©í–ˆë”ë¼ë„, [ë§¤ë‰´ì–¼ ë°ì´í„°]ì— í•´ë‹¹ ì œí’ˆêµ°(ì˜ˆ: ì¼ë°˜ ì„¸íƒê¸°, ë“œëŸ¼ ì„¸íƒê¸°)ì— ëŒ€í•œ ë‚´ìš©ì´ ìˆë‹¤ë©´ ê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
    4. user_inputì— 'ëµí' ë¼ëŠ” ë‹¨ì–´ê°€ ë“¤ì–´ê°„ ë¬¸ì¥ì´ ë“¤ì–´ì˜¤ë©´ ê·¸ ë¬¸ì¥ì— 'ëµí' ë¥¼ 'LG ThinQ' ë¡œ ë³€ê²½í•˜ê³  ë‹µë³€í•´ì¤˜ì¤˜
    [ë§¤ë‰´ì–¼ ë°ì´í„°]:
    {context_text}
    
    [ì‚¬ìš©ì ì§ˆë¬¸]:
    {query_text}
    
    [ë‹µë³€]:
    """
    
    response = generation_model.generate_content(prompt)
    return response.text

def main():
    print("ğŸ¤– [í•˜ì´ë¸Œë¦¬ë“œ] ê°€ì „ì œí’ˆ AI ì±—ë´‡ (ì¢…ë£Œ: 'exit')")
    print("-" * 50)
    
    while True:
        user_input = input("\nì§ˆë¬¸í•˜ì„¸ìš”: ")
        if user_input.lower() == 'exit':
            break
            
        print("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(ë²¡í„°+í‚¤ì›Œë“œ) ì¤‘...")
        
        # 1. ê²€ìƒ‰
        search_results = search_manual(user_input)
        
        if search_results:
            # SQLì—ì„œ section_titleì„ ì§ì ‘ selectí–ˆìœ¼ë¯€ë¡œ metadata ì—†ì´ ë°”ë¡œ ì ‘ê·¼ ê°€ëŠ¥
            titles = [f"{item.get('section_title')} (ìœ ì‚¬ë„: {item.get('similarity'):.2f})" for item in search_results]
            print(f"   => ì°¸ê³ í•œ ì„¹ì…˜: {titles}")
        else:
            print("   => âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

        # 2. ë‹µë³€ ìƒì„±
        answer = generate_answer(user_input, search_results)
        
        print("\nğŸ’¬ AI ë‹µë³€:")
        print(answer)
        print("-" * 50)

if __name__ == "__main__":
    main()