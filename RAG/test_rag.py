import os
import google.generativeai as genai
from supabase import create_client, Client

# ==========================================
# 1. ì„¤ì • ì •ë³´ (upload_manual.pyì™€ ë™ì¼í•˜ê²Œ ì…ë ¥)
# ==========================================
SUPABASE_URL = "https://wzafalbctqkylhyzlfej.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Ind6YWZhbGJjdHFreWxoeXpsZmVqIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2NDAzNTM5NywiZXhwIjoyMDc5NjExMzk3fQ.Ax6HgxBruVRbUIhYtmDKK1yW8OkoSGjFg3GLupS91uI" # í˜¹ì€ ANON KEY
GOOGLE_API_KEY = "AIzaSyCE8-7jyJBbugZX6GRCMvGhPfBtkZeXXY0"
# ==========================================

# API ì´ˆê¸°í™”
genai.configure(api_key=GOOGLE_API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ë‹µë³€ì„ ìƒì„±í•  Gemini ëª¨ë¸ ì„¤ì • (ë¹ ë¥´ê³  ë˜‘ë˜‘í•œ 2.5 Flash ì¶”ì²œ)
generation_model = genai.GenerativeModel('gemini-2.5-flash')

def get_embedding(text):
    """ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜"""
    result = genai.embed_content(
        model="models/text-embedding-004",
        content=text,
        task_type="retrieval_query" # ë¬¸ì„œë¥¼ ì°¾ê¸° ìœ„í•œ ì§ˆë¬¸ìš© íƒ€ì…
    )
    return result['embedding']

def search_manual(query_text):
    """DBì—ì„œ ìœ ì‚¬í•œ ë§¤ë‰´ì–¼ ë‚´ìš© ê²€ìƒ‰ (RAG - Retrieval)"""
    
    # 1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
    query_vector = get_embedding(query_text)
    
    # 2. Supabase RPC í•¨ìˆ˜ í˜¸ì¶œ (ë²¡í„° ê²€ìƒ‰)
    response = supabase.rpc("match_manual_sections", {
        "query_embedding": query_vector,
        "match_threshold": 0.1, # ìœ ì‚¬ë„ 50% ì´ìƒë§Œ (ë„ˆë¬´ ë‚®ìœ¼ë©´ ì—‰ëš±í•œ ê±° ê°€ì ¸ì˜´)
        "match_count": 5        # ê°€ì¥ ë¹„ìŠ·í•œ ë‚´ìš© 3ê°œë§Œ ê°€ì ¸ì˜¤ê¸°
    }).execute()
    
    return response.data

def generate_answer(query_text, context_list):
    """ê²€ìƒ‰ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„± (RAG - Generation)"""
    
    if not context_list:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë§¤ë‰´ì–¼ì—ì„œ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ê²€ìƒ‰ëœ í…ìŠ¤íŠ¸ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹¨
    context_text = "\n\n".join([f"- {item['content_text']}" for item in context_list])

    # Geminiì—ê²Œ ì¤„ í”„ë¡¬í”„íŠ¸ (í˜ë¥´ì†Œë‚˜ ë¶€ì—¬)
    prompt = f"""
    ë‹¹ì‹ ì€ LG ìŠ¤íƒ ë“œ ì—ì–´ì»¨ ì‚¬ìš©ì„ ë„ì™€ì£¼ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ì•„ë˜ ì œê³µëœ [ë§¤ë‰´ì–¼ ë‚´ìš©]ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ì— ë‹µë³€í•˜ì„¸ìš”.
    ë©”ë‰´ì–¼ì— ì œê³µë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ë©”ë‰´ì–¼ì— ì—†ëŠ” ë‚´ìš©ì´ë¼ê³  ë‹µë³€í•´
    
    
    [ë§¤ë‰´ì–¼ ë‚´ìš©]:
    {context_text}
    
    [ì§ˆë¬¸]:
    {query_text}
    
    [ë‹µë³€]:
    """
    
    # AI ë‹µë³€ ìƒì„±
    response = generation_model.generate_content(prompt)
    return response.text

def main():
    print("ğŸ¤– ì—ì–´ì»¨ AI ì±—ë´‡ í…ŒìŠ¤íŠ¸ (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)")
    print("-" * 50)
    
    while True:
        user_input = input("\nì§ˆë¬¸í•˜ì„¸ìš”: ")
        if user_input.lower() == 'exit':
            break
            
        print("ğŸ” ë§¤ë‰´ì–¼ ê²€ìƒ‰ ì¤‘...")
        
        # 1. DB ê²€ìƒ‰
        search_results = search_manual(user_input)
        
        if search_results:
            print(f"   => ì°¸ê³ í•œ ë§¤ë‰´ì–¼ ì„¹ì…˜: {[item['section_title'] for item in search_results]}")
        else:
            print("   => âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ìœ ì‚¬ë„ ë‚®ìŒ)")
            
        # 2. ë‹µë³€ ìƒì„±
        answer = generate_answer(user_input, search_results)
        
        print("\nğŸ’¬ AI ë‹µë³€:")
        print(answer)
        print(context_text)
        print("-" * 50)

if __name__ == "__main__":
    main()