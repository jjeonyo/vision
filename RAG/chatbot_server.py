from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import os
from dotenv import load_dotenv
from supabase import create_client, Client
import google.generativeai as genai

# ==========================================
# 1. ì„¤ì • ë° ì´ˆê¸°í™”
# ==========================================
load_dotenv()
SUPABASE_URL = "https://wzafalbctqkylhyzlfej.supabase.co"
SUPABASE_KEY = os.getenv("supbase_service_role")
GOOGLE_API_KEY = os.getenv("google_api")

# í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ ìƒì„±
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
genai.configure(api_key=GOOGLE_API_KEY)

# ëª¨ë¸ ì„¤ì •
embedding_model = "models/text-embedding-004"
generation_model = genai.GenerativeModel("gemini-2.5-flash")

# ==========================================
# 2. FastAPI ì„œë²„ ì„¤ì •
# ==========================================
app = FastAPI()

class ChatRequest(BaseModel):
    user_message: str
    user_id: str

class ChatResponse(BaseModel):
    answer: str
    sources: List[str]

# ğŸ”¥ í•µì‹¬ ë¡œì§: ë­ì²´ì¸ ì—†ì´ ì§ì ‘ êµ¬í˜„ (ì—ëŸ¬ ì›ì²œ ì°¨ë‹¨)
@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(req: ChatRequest):
    print(f"ğŸ“© [Spring -> Python] ìš”ì²­ ë„ì°©: {req.user_message}")
    
    try:
        # 1. ì§ˆë¬¸ì„ ë²¡í„°(ìˆ«ì)ë¡œ ë³€í™˜
        emb_result = genai.embed_content(
            model=embedding_model,
            content=req.user_message,
            task_type="retrieval_query"
        )
        query_vector = emb_result['embedding']

        # 2. DB í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ (ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ì•ˆ ë‚˜ê²Œ ë¨!)
        #    ì•„ê¹Œ ë§Œë“  SQL í•¨ìˆ˜ì˜ íŒŒë¼ë¯¸í„° ì´ë¦„ê³¼ ì •í™•íˆ ì¼ì¹˜ì‹œí‚µë‹ˆë‹¤.
        rpc_response = supabase.rpc("match_manual_sections", {
            "query_embedding": query_vector,
            "match_threshold": 0.3,
            "match_count": 3
        }).execute()
        
        # 3. ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬
        search_results = rpc_response.data
        
        if not search_results:
            print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return ChatResponse(answer="ë§¤ë‰´ì–¼ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", sources=[])

        # ê²€ìƒ‰ëœ ë‚´ìš©ì„ í•˜ë‚˜ë¡œ í•©ì¹¨
        context_text = "\n\n".join([f"- {item['content']}" for item in search_results])
        source_titles = list(set([item['section_title'] for item in search_results]))

        # 4. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
        ë‹¹ì‹ ì€ LG ThinQ ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ [ë§¤ë‰´ì–¼ ë‚´ìš©]ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µí•˜ì„¸ìš”.
        
        [ë§¤ë‰´ì–¼ ë‚´ìš©]:
        {context_text}
        
        [ì§ˆë¬¸]: {req.user_message}
        
        [ë‹µë³€]:
        """

        # 5. ë‹µë³€ ìƒì„±
        gen_response = generation_model.generate_content(prompt)
        final_answer = gen_response.text

        print(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ: {final_answer[:30]}...")

        return ChatResponse(
            answer=final_answer,
            sources=source_titles
        )

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        # ì—ëŸ¬ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ë³´ì—¬ì¤˜ì„œ ë””ë²„ê¹…ì„ ë•ìŠµë‹ˆë‹¤.
        return ChatResponse(
            answer=f"ì„œë²„ ì—ëŸ¬ ë°œìƒ: {str(e)}",
            sources=[]
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)