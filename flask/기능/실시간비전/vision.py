import asyncio
import os
import cv2
import pathlib
import sys
import time
import pyaudio
import warnings
import traceback
import threading
import queue
import speech_recognition as sr
import audioop
import sqlite3
import textwrap
from dotenv import load_dotenv

# [ìˆ˜ì •] google.genaiì—ì„œ types ì„í¬íŠ¸
try:
    from google import genai
    from google.genai import types
except ImportError:
    print("âŒ google-genai ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    sys.exit(1)

# [ì„¤ì •] ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore")

# ==========================================
# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜
# ==========================================

def load_environment():
    try:
        current_dir = pathlib.Path(__file__).parent.absolute()
        env_path = None
        for parent in [current_dir] + list(current_dir.parents):
            check_path = parent / ".env"
            if check_path.exists():
                env_path = check_path
                break

        if env_path:
            load_dotenv(dotenv_path=env_path)
        else:
            print("âš ï¸ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ .env ë¡œë“œ ì˜¤ë¥˜: {e}")

load_environment()

API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    print("âŒ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    sys.exit(1)


MODEL_ID = "gemini-2.5-flash-native-audio-preview-09-2025"
#MODEL_ID = "gemini-2.5-flash"
#MODEL_ID = "gemini-2.5-flash-preview-09-2025"
#MODEL_ID = "gemini-2.0-flash"
#MODEL_ID = "gemini-2.0-flash-exp"

# [ì˜¤ë””ì˜¤ ì„¤ì •]
AUDIO_FORMAT = pyaudio.paInt16
CHANNELS = 1
INPUT_RATE = 16000
OUTPUT_RATE = 24000
CHUNK_SIZE = 512
MIC_DEVICE_INDEX = None

# ==========================================
# [í•¨ìˆ˜] ì„¤ì • ë° í˜ë¥´ì†Œë‚˜ ë¡œë“œ
# ==========================================

def get_config():
    current_dir = pathlib.Path(__file__).parent.absolute()
    persona_path = current_dir / "persona.txt"
    
    system_instruction = ""
    if persona_path.exists():
        try:
            system_instruction = persona_path.read_text(encoding="utf-8")
            print(f"ğŸ­ í˜ë¥´ì†Œë‚˜ ë¡œë“œë¨: {persona_path.name}")
        except Exception:
            pass
    else:
        system_instruction = "ë„ˆëŠ” ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. ì‹¤ì‹œê°„ìœ¼ë¡œ ëŒ€í™”í•´."

    return {
        "response_modalities": ["AUDIO"],
        "speech_config": {
            "voice_config": {
                "prebuilt_voice_config": {
                    "voice_name": "Aoede"
                }
            }
        },
        "system_instruction": system_instruction
    }

# ==========================================
# [í´ë˜ìŠ¤] DB ë¡œê·¸ ì €ì¥ (SQLite)
# ==========================================
class DatabaseLogger:
    def __init__(self, db_path="chat_history.db"):
        self.db_path = db_path
        self.buffer = []
        self.session_id = None
        self._init_db()
        self._start_session()

    def _init_db(self):
        """DB í…Œì´ë¸” ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            # ì„¸ì…˜ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS sessions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    start_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    model_id TEXT
                )
            ''')
            # ë©”ì‹œì§€ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS messages (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id INTEGER,
                    sender TEXT,
                    content TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (session_id) REFERENCES sessions (id)
                )
            ''')
            conn.commit()

    def _start_session(self):
        """ìƒˆë¡œìš´ ëŒ€í™” ì„¸ì…˜ ì‹œì‘"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('INSERT INTO sessions (model_id) VALUES (?)', (MODEL_ID,))
            self.session_id = cursor.lastrowid
            conn.commit()
        print(f"ğŸ’¾ DB ì„¸ì…˜ ì‹œì‘ë¨: ID {self.session_id}")

    def append_text(self, text):
        self.buffer.append(text)

    def log_user_message(self, text):
        """ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO messages (session_id, sender, content) 
                    VALUES (?, ?, ?)
                ''', (self.session_id, 'user', text))
                conn.commit()
        except Exception as e:
            print(f"\nâš ï¸ DB ì €ì¥ ì‹¤íŒ¨ (User): {e}")

    def flush_model_turn(self):
        """ëª¨ë¸ ì‘ë‹µ ì €ì¥"""
        if not self.buffer: return
        
        full_text = "".join(self.buffer)
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO messages (session_id, sender, content) 
                    VALUES (?, ?, ?)
                ''', (self.session_id, 'gemini', full_text))
                conn.commit()
        except Exception as e:
            print(f"\nâš ï¸ DB ì €ì¥ ì‹¤íŒ¨ (Gemini): {e}")
            
        self.buffer = []

# ==========================================
# [í´ë˜ìŠ¤] STT ì²˜ë¦¬ê¸° (ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ)
# ==========================================
class SpeechTranscriber:
    def __init__(self, logger, shared_state=None):
        self.logger = logger
        self.shared_state = shared_state
        self.audio_queue = queue.Queue()
        self.running = True
        self.recognizer = sr.Recognizer()
        self.thread = threading.Thread(target=self._process_loop, daemon=True)
        self.thread.start()
        
        # STT ì„¤ì •
        self.energy_threshold = 1000  # ìŒì„± ê°ì§€ ì„ê³„ê°’ (ì¡°ì ˆ í•„ìš”)
        self.pause_threshold = 0.8    # ë§ ëŠê¹€ ê°„ì£¼ ì‹œê°„ (ì´ˆ)
        self.sample_rate = 16000
        self.sample_width = 2         # 16-bit = 2 bytes
    
    def add_audio(self, data):
        if self.running:
            self.audio_queue.put(data)
            
    def stop(self):
        self.running = False
        self.thread.join(timeout=1.0)

    def _process_loop(self):
        print("ğŸ‘‚ STT ë¦¬ìŠ¤ë„ˆ ì‹œì‘ (í•œêµ­ì–´)")
        
        audio_buffer = bytearray()
        silence_frames = 0
        has_voice = False
        
        # 1 í”„ë ˆì„(ì²­í¬) ë‹¹ ì‹œê°„ ê³„ì‚°
        # CHUNK_SIZE(512) / RATE(16000) = 0.032ì´ˆ
        chunk_duration = 512 / 16000
        pause_frame_count = int(self.pause_threshold / chunk_duration)
        
        while self.running:
            try:
                # íì—ì„œ ì˜¤ë””ì˜¤ ì²­í¬ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ 1ì´ˆ)
                data = self.audio_queue.get(timeout=1.0)
                
                # ì—ë„ˆì§€(ì†Œë¦¬ í¬ê¸°) ê³„ì‚°
                rms = audioop.rms(data, self.sample_width)
                
                if rms > self.energy_threshold:
                    has_voice = True
                    silence_frames = 0
                else:
                    if has_voice:
                        silence_frames += 1
                
                # ë²„í¼ì— ë°ì´í„° ì¶”ê°€
                if has_voice:
                    audio_buffer.extend(data)
                
                # ë§ì´ ëë‚¬ë‹¤ê³  íŒë‹¨ë˜ë©´ (ì¼ì • ì‹œê°„ ì¹¨ë¬µ)
                if has_voice and silence_frames > pause_frame_count:
                    # ì¸ì‹ ìˆ˜í–‰
                    self._recognize(audio_buffer)
                    
                    # ì´ˆê¸°í™”
                    audio_buffer = bytearray()
                    silence_frames = 0
                    has_voice = False
                    
                # ë²„í¼ê°€ ë„ˆë¬´ ì»¤ì§€ë©´ (ì˜ˆ: 15ì´ˆ ì´ìƒ) ê°•ì œ ì¸ì‹ (ë©”ëª¨ë¦¬ ë³´í˜¸)
                if len(audio_buffer) > 16000 * 2 * 15:
                    self._recognize(audio_buffer)
                    audio_buffer = bytearray()
                    silence_frames = 0
                    has_voice = False

            except queue.Empty:
                continue
            except Exception as e:
                print(f"STT ë£¨í”„ ì˜¤ë¥˜: {e}")
                
    def _recognize(self, audio_data):
        if len(audio_data) < 16000 * 2 * 0.5: # 0.5ì´ˆ ë¯¸ë§Œì€ ë¬´ì‹œ
            return
            
        try:
            # Raw PCM ë°ì´í„°ë¥¼ AudioData ê°ì²´ë¡œ ë³€í™˜
            audio_source = sr.AudioData(bytes(audio_data), self.sample_rate, self.sample_width)
            
            # Google Web Speech API í˜¸ì¶œ (ë™ê¸°)
            text = self.recognizer.recognize_google(audio_source, language="ko-KR")
            if text.strip():
                print(f"\n[ğŸ—£ï¸ User]: {text}")
                self.logger.log_user_message(text)
                
                # [ì¶”ê°€] ì‚¬ìš©ìê°€ ë§í•˜ë©´ ë§í’ì„  ì´ˆê¸°í™” (ëŒ€í™” ëŠë‚Œ)
                # shared_state ì ‘ê·¼ì´ ì–´ë ¤ìš°ë¯€ë¡œ ë¡œê±°ë¥¼ í†µí•´ ìš°íšŒí•˜ê±°ë‚˜ ì „ì—­ ë³€ìˆ˜ ê³ ë ¤
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì „ì—­ shared_stateê°€ ì—†ìœ¼ë¯€ë¡œ ìƒëµí•˜ê±°ë‚˜ 
                # SpeechTranscriberì— shared_state ì°¸ì¡°ë¥¼ ë„˜ê²¨ì£¼ëŠ” ê²ƒì´ ì¢‹ìŒ
                if hasattr(self, 'shared_state') and self.shared_state:
                     self.shared_state["display_text"] = "..."
                
        except sr.UnknownValueError:
            # ì¸ì‹ ì‹¤íŒ¨ (ì¡ìŒ ë“±) - ì¡°ìš©íˆ ë„˜ì–´ê°
            pass
        except sr.RequestError as e:
            print(f"STT API ì˜¤ë¥˜: {e}")
        except Exception as e:
            print(f"STT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ==========================================
# [ë©”ì¸] ì‹¤í–‰ ë£¨í”„
# ==========================================

async def main():
    try:
        client = genai.Client(api_key=API_KEY)
        config = get_config()
        p = pyaudio.PyAudio()
        
        input_stream = None
        output_stream = None

        try:
            output_stream = p.open(format=AUDIO_FORMAT, channels=CHANNELS, rate=OUTPUT_RATE, output=True)
            input_stream = p.open(format=AUDIO_FORMAT, channels=CHANNELS, rate=INPUT_RATE, input=True, 
                                  input_device_index=MIC_DEVICE_INDEX, frames_per_buffer=CHUNK_SIZE)
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            return

        cap = cv2.VideoCapture(0)
        # ë‚´ í™”ë©´ìš© í•´ìƒë„ (ê³ í•´ìƒë„)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)

        if not cap.isOpened():
            print("âŒ ì›¹ìº ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"\nğŸš€ ëª¨ë¸({MODEL_ID}) ì—°ê²° ì¤‘...")

        # ìºë¦­í„° ì´ë¯¸ì§€ ë¡œë“œ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
        # ê²½ë¡œ: flask/ê¸°ëŠ¥/ì´ë¯¸ì§€ìƒì„±/assets_generate/result_solution_20251126_114052.png (ì˜ˆì‹œ)
        global character_img
        character_img = None
        try:
            # í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ìƒìœ„ë¡œ ì´ë™í•˜ì—¬ ì—ì…‹ ì°¾ê¸°
            base_dir = pathlib.Path(__file__).parent.parent.parent / "ê¸°ëŠ¥" / "ì´ë¯¸ì§€ìƒì„±" / "assets_generate"
            # ê°€ì¥ ìµœì‹  íŒŒì¼ í•˜ë‚˜ ì„ íƒ ì˜ˆì‹œ
            char_path = base_dir / "result_solution_20251126_114052.png"
            
            if char_path.exists():
                character_img = cv2.imread(str(char_path), cv2.IMREAD_UNCHANGED)
                print(f"âœ… ìºë¦­í„° ì´ë¯¸ì§€ ë¡œë“œë¨: {char_path.name}")
            else:
                print("âš ï¸ ìºë¦­í„° ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ ìºë¦­í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

        # ê³µìœ  ë°ì´í„° ì»¨í…Œì´ë„ˆ (ë¯¸ë¦¬ ì •ì˜í•˜ì—¬ STTì— ì „ë‹¬)
        shared_state = {
            "latest_frame": None, 
            "running": True,
            "display_text": "ì•ˆë…•í•˜ì„¸ìš”!" 
        }

        logger = DatabaseLogger()
        stt_transcriber = SpeechTranscriber(logger, shared_state)

        try:
            async with client.aio.live.connect(model=MODEL_ID, config=config) as session:
                print("âœ… ì—°ê²° ì„±ê³µ! ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”. (ì¢…ë£Œ: Ctrl+C ë˜ëŠ” í™”ë©´ì—ì„œ 'q')")
                
                # -------------------------------------------------------
                # [Task 1] ë¹„ë””ì˜¤ ì²˜ë¦¬ (í™”ë©´ í‘œì‹œ + ì „ì†¡ ë¶„ë¦¬)
                # -------------------------------------------------------
                # shared_stateëŠ” ìœ„ì—ì„œ ì •ì˜ë¨

                # ==========================================
                # [í•¨ìˆ˜] ì´ë¯¸ì§€ ì˜¤ë²„ë ˆì´ (íˆ¬ëª… ë°°ê²½ ì§€ì›)
                # ==========================================
                def overlay_image(background, overlay, x, y, overlay_size=None):
                    try:
                        h, w = background.shape[:2]
                        
                        if overlay_size:
                            overlay = cv2.resize(overlay, overlay_size)
                        
                        h_overlay, w_overlay = overlay.shape[:2]
                        
                        # ê²½ê³„ ì²´í¬
                        if x + w_overlay > w: w_overlay = w - x
                        if y + h_overlay > h: h_overlay = h - y
                        if w_overlay <= 0 or h_overlay <= 0: return background

                        overlay_crop = overlay[:h_overlay, :w_overlay]
                        background_crop = background[y:y+h_overlay, x:x+w_overlay]

                        # ì•ŒíŒŒ ì±„ë„ í™•ì¸ (íˆ¬ëª…ë„)
                        if overlay_crop.shape[2] == 4:
                            alpha = overlay_crop[:, :, 3] / 255.0
                            alpha_inv = 1.0 - alpha
                            
                            for c in range(3):
                                background_crop[:, :, c] = (alpha * overlay_crop[:, :, c] + 
                                                            alpha_inv * background_crop[:, :, c])
                        else:
                            background_crop[:] = overlay_crop

                        background[y:y+h_overlay, x:x+w_overlay] = background_crop
                        return background
                    except Exception as e:
                        # print(f"ì˜¤ë²„ë ˆì´ ì˜¤ë¥˜: {e}")
                        return background

                async def capture_and_display():
                    print("ğŸ“· ì¹´ë©”ë¼ ìº¡ì²˜ ì‹œì‘")
                    while shared_state["running"]:
                        ret, frame = cap.read()
                        if not ret: 
                            print("âŒ ì¹´ë©”ë¼ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                            break

                        # [ì¤‘ìš”] ìºë¦­í„°ê°€ í•©ì„±ë˜ì§€ ì•Šì€ ìˆœìˆ˜ ì›ë³¸ í”„ë ˆì„ì„ ì „ì†¡ìš©ìœ¼ë¡œ ì €ì¥
                        shared_state["latest_frame"] = frame.copy()
                        
                        # [í•¨ìˆ˜] ë§í’ì„  ê·¸ë¦¬ê¸° (ì´ì „ ì •ì˜ëœ overlay_image ì‚¬ìš©)
                        def draw_speech_bubble(img, text, x, y, char_w, char_h):
                            if not text: return img
                            
                            # í…ìŠ¤íŠ¸ ë˜í•‘ (í•œ ì¤„ì— ì•½ 20ì)
                            wrapped_lines = textwrap.wrap(text, width=20)
                            if len(wrapped_lines) > 4: # ë„ˆë¬´ ê¸¸ë©´ ìµœê·¼ 4ì¤„ë§Œ
                                wrapped_lines = wrapped_lines[-4:]
                                
                            # í°íŠ¸ ì„¤ì •
                            font = cv2.FONT_HERSHEY_SIMPLEX
                            font_scale = 0.6
                            thickness = 2
                            padding = 15
                            line_height = 30
                            
                            # ë°•ìŠ¤ í¬ê¸° ê³„ì‚°
                            text_w = 0
                            for line in wrapped_lines:
                                size = cv2.getTextSize(line, font, font_scale, thickness)[0]
                                if size[0] > text_w: text_w = size[0]
                            
                            box_w = text_w + (padding * 2)
                            box_h = (len(wrapped_lines) * line_height) + (padding * 2)
                            
                            # ë§í’ì„  ìœ„ì¹˜ (ìºë¦­í„° ë¨¸ë¦¬ ìœ„)
                            # ìºë¦­í„° ìœ„ì¹˜ê°€ (x, y)ì´ë¯€ë¡œ, ê·¸ ìœ„ìª½
                            bubble_x = x + (char_w // 2) - (box_w // 2)
                            bubble_y = y - box_h - 20
                            
                            # í™”ë©´ ë°–ìœ¼ë¡œ ë‚˜ê°€ì§€ ì•Šê²Œ ì¡°ì •
                            if bubble_x < 10: bubble_x = 10
                            if bubble_x + box_w > img.shape[1] - 10: bubble_x = img.shape[1] - box_w - 10
                            if bubble_y < 10: bubble_y = 10 # í™”ë©´ ìœ„ìª½ ì§¤ë¦¼ ë°©ì§€

                            # ë§í’ì„  ë°°ê²½ (í°ìƒ‰)
                            cv2.rectangle(img, (bubble_x, bubble_y), (bubble_x + box_w, bubble_y + box_h), (255, 255, 255), -1)
                            cv2.rectangle(img, (bubble_x, bubble_y), (bubble_x + box_w, bubble_y + box_h), (0, 0, 0), 2)
                            
                            # ê¼¬ë¦¬ ê·¸ë¦¬ê¸° (ì§€ì‹œì„ )
                            cv2.line(img, (bubble_x + box_w//2, bubble_y + box_h), (x + char_w//2, y), (0,0,0), 2)

                            # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
                            for i, line in enumerate(wrapped_lines):
                                text_y = bubble_y + padding + (i + 1) * line_height - 10
                                cv2.putText(img, line, (bubble_x + padding, text_y), font, font_scale, (0, 0, 0), thickness)
                            
                            return img

                        cv2.imshow('Gemini Live Vision', frame)
                        if cv2.waitKey(1) & 0xFF == ord('q'):
                            shared_state["running"] = False
                            break
                        
                        # [ì¶”ê°€] ìºë¦­í„° ì˜¤ë²„ë ˆì´ (ìš°ì¸¡ í•˜ë‹¨)
                        if character_img is not None:
                            # í™”ë©´ í¬ê¸°ì— ë§ì¶° ë¦¬ì‚¬ì´ì¦ˆ (ë„ˆë¹„ì˜ 20% ì •ë„)
                            screen_h, screen_w = frame.shape[:2]
                            char_w = int(screen_w * 0.2)
                            char_h = int(char_w * (character_img.shape[0] / character_img.shape[1]))
                            
                            # ìš°ì¸¡ í•˜ë‹¨ ì¢Œí‘œ ê³„ì‚° (ì—¬ë°± 20px)
                            pos_x = screen_w - char_w - 20
                            pos_y = screen_h - char_h - 20
                            
                            frame = overlay_image(frame, character_img, pos_x, pos_y, (char_w, char_h))
                            
                            # [ì¶”ê°€] ë§í’ì„  ê·¸ë¦¬ê¸°
                            if shared_state["display_text"]:
                                frame = draw_speech_bubble(frame, shared_state["display_text"], pos_x, pos_y, char_w, char_h)

                        cv2.imshow('Gemini Live Vision', frame)
                        if cv2.waitKey(1) & 0xFF == ord('q'):
                            shared_state["running"] = False
                            break
                        
                        # í™”ë©´ ê°±ì‹  (ì•½ 30 FPS)
                        await asyncio.sleep(0.03)
                async def send_video_frames():
                    print("ğŸ“¡ ë¹„ë””ì˜¤ ì „ì†¡ ë°ëª¬ ì‹œì‘")
                    while shared_state["running"]:
                        if shared_state["latest_frame"] is not None:
                            frame = shared_state["latest_frame"]
                            
                            # ì „ì†¡ ê·œê²© 640x480
                            frame_resized = cv2.resize(frame, (640, 480))
                            _, buffer = cv2.imencode('.jpg', frame_resized, [int(cv2.IMWRITE_JPEG_QUALITY), 50])
                            
                            try:
                                await session.send_realtime_input(
                                    video=types.Blob(
                                        data=buffer.tobytes(), 
                                        mime_type="image/jpeg"
                                    )
                                )
                            except TypeError:
                                await session.send_realtime_input(
                                    data=buffer.tobytes(), 
                                    mime_type="image/jpeg"
                                )
                            except Exception as e:
                                print(f"ë¹„ë””ì˜¤ ì „ì†¡ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
                        
                        # ì „ì†¡ ì£¼ê¸° (0.4ì´ˆ = 2.5 FPS)
                        await asyncio.sleep(0.4)
                
                # -------------------------------------------------------
                # [Task 2] ì˜¤ë””ì˜¤ ì…ë ¥
                # -------------------------------------------------------
                async def send_audio_stream():
                    print("ğŸ™ï¸ ë§ˆì´í¬ ì „ì†¡ ì‹œì‘")
                    while True:
                        try:
                            data = await asyncio.to_thread(input_stream.read, CHUNK_SIZE, exception_on_overflow=False)
                            
                            # STT ì²˜ë¦¬ë¥¼ ìœ„í•´ ë°ì´í„° ë³µì‚¬ë³¸ ì „ë‹¬
                            stt_transcriber.add_audio(data)
                            
                            # [ìˆ˜ì •] ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ audio=types.Blob(...) í˜•íƒœë¡œ ì „ì†¡
                            await session.send_realtime_input(
                                audio=types.Blob(
                                    data=data, 
                                    mime_type="audio/pcm;rate=16000"
                                )
                            )
                        except Exception as e:
                            print(f"ì˜¤ë””ì˜¤ ì „ì†¡ ì˜¤ë¥˜: {e}")
                            break

                # -------------------------------------------------------
                # [Task 3] ì‘ë‹µ ìˆ˜ì‹ 
                # -------------------------------------------------------
                async def receive_response():
                    while True:
                        try:
                            async for response in session.receive():
                                if response.server_content:
                                    model_turn = response.server_content.model_turn
                                    if model_turn:
                                        for part in model_turn.parts:
                                            if part.inline_data:
                                                output_stream.write(part.inline_data.data)
                                            if part.text:
                                                print(part.text, end="", flush=True)
                                                logger.append_text(part.text)
                                                
                                                # [ì¶”ê°€] ë§í’ì„  í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                                                if "display_text" not in shared_state: 
                                                    shared_state["display_text"] = ""
                                                # ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ì´ˆê¸°í™” (ìƒˆë¡œìš´ í„´ ê°ì§€ ë¡œì§ ëŒ€ì‹  ë‹¨ìˆœ ê¸¸ì´ ì œí•œ)
                                                if len(shared_state["display_text"]) > 50:
                                                    shared_state["display_text"] = part.text
                                                else:
                                                    shared_state["display_text"] += part.text
                                    
                                    # í„´ì´ ëë‚¬ëŠ”ì§€ í™•ì¸ (API ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                                    # turn_completeê°€ ëª…ì‹œì ìœ¼ë¡œ ì˜¤ë©´ ì €ì¥
                                    if getattr(response.server_content, "turn_complete", False):
                                        logger.flush_model_turn()
                                        # ë§í’ì„  í…ìŠ¤íŠ¸ëŠ” ìœ ì§€ (ì‚¬ìš©ìê°€ ì½ì„ ì‹œê°„ í™•ë³´)
                                        
                        except Exception as e:
                            print(f"ìˆ˜ì‹  ì˜¤ë¥˜: {e}")
                            break

                video_display_task = asyncio.create_task(capture_and_display())
                video_sender_task = asyncio.create_task(send_video_frames())
                audio_task = asyncio.create_task(send_audio_stream())
                recv_task = asyncio.create_task(receive_response())

                try:
                    # ì¹´ë©”ë¼ ì°½ì´ ë‹«í ë•Œê¹Œì§€ ëŒ€ê¸°
                    await video_display_task
                except asyncio.CancelledError:
                    pass
                finally:
                    video_sender_task.cancel()
                    audio_task.cancel()
                    recv_task.cancel()

        except Exception as e:
            print(f"\nâŒ ì„¸ì…˜ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
        finally:
            stt_transcriber.stop()
            
            # [ì¢…ë£Œ ì‹œí€€ìŠ¤] ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘
            print("\n" + "="*40)
            print("ğŸ‘‹ ìƒë‹´ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            try:
                feedback = input("ğŸ’¡ ì´ë²ˆ ìƒë‹´ì´ ë„ì›€ì´ ë˜ì…¨ë‚˜ìš”? (y/n): ").strip().lower()
                feedback_score = 1 if feedback == 'y' else 0
                
                # ë§ˆì§€ë§‰ ì„¸ì…˜ ID ê°€ì ¸ì˜¤ê¸° ë° í”¼ë“œë°± ì—…ë°ì´íŠ¸
                if logger.session_id:
                    with sqlite3.connect(logger.db_path) as conn:
                        cursor = conn.cursor()
                        # sessions í…Œì´ë¸”ì— feedback ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ ì¶”ê°€ (ë§ˆì´ê·¸ë ˆì´ì…˜)
                        try:
                            cursor.execute('ALTER TABLE sessions ADD COLUMN feedback INTEGER')
                        except sqlite3.OperationalError:
                            pass # ì´ë¯¸ ì¡´ì¬í•¨
                            
                        cursor.execute('UPDATE sessions SET feedback = ? WHERE id = ?', 
                                     (feedback_score, logger.session_id))
                        conn.commit()
                    print("âœ… í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
            except Exception as e:
                print(f"í”¼ë“œë°± ì €ì¥ ì˜¤ë¥˜: {e}")
            print("="*40 + "\n")

            if cap.isOpened(): cap.release()
            if input_stream: input_stream.stop_stream(); input_stream.close()
            if output_stream: output_stream.stop_stream(); output_stream.close()
            if p: p.terminate()
            cv2.destroyAllWindows()

    except Exception as e:
        print(f"\nâŒ ë©”ì¸ ì˜¤ë¥˜: {e}")
        input("ì—”í„°ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œí•©ë‹ˆë‹¤...")

if __name__ == "__main__":
    asyncio.run(main())
