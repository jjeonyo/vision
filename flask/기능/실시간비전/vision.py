import asyncio
import os
import cv2
import pathlib
import sys
import time
import pyaudio
import warnings
import traceback
import threading
import queue
import speech_recognition as sr
import audioop
import sqlite3
import textwrap
from dotenv import load_dotenv

# [ìˆ˜ì •] google.genaiì—ì„œ types ìž„í¬íŠ¸
try:
    from google import genai
    from google.genai import types
except ImportError:
    print("âŒ google-genai ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    sys.exit(1)

# [ì„¤ì •] ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore")

# ==========================================
# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜
# ==========================================

def load_environment():
    try:
        current_dir = pathlib.Path(__file__).parent.absolute()
        env_path = None
        for parent in [current_dir] + list(current_dir.parents):
            check_path = parent / ".env"
            if check_path.exists():
                env_path = check_path
                break

        if env_path:
            load_dotenv(dotenv_path=env_path)
        else:
            print("âš ï¸ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ .env ë¡œë“œ ì˜¤ë¥˜: {e}")

load_environment()

API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    print("âŒ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    sys.exit(1)


MODEL_ID = "gemini-2.5-flash-native-audio-preview-09-2025"
#MODEL_ID = "gemini-2.5-flash"
#MODEL_ID = "gemini-2.5-flash-preview-09-2025"
#MODEL_ID = "gemini-2.0-flash"
#MODEL_ID = "gemini-2.0-flash-exp"

# [ì˜¤ë””ì˜¤ ì„¤ì •]
AUDIO_FORMAT = pyaudio.paInt16
CHANNELS = 1
INPUT_RATE = 16000
OUTPUT_RATE = 24000
CHUNK_SIZE = 512
MIC_DEVICE_INDEX = None

# ==========================================
# [í•¨ìˆ˜] ì„¤ì • ë° íŽ˜ë¥´ì†Œë‚˜ ë¡œë“œ
# ==========================================

def get_config():
    current_dir = pathlib.Path(__file__).parent.absolute()
    persona_path = current_dir / "persona_ì„¸íƒê¸°ìˆ˜ë¦¬ë²•.txt"
    
    system_instruction = ""
    if persona_path.exists():
        try:
            system_instruction = persona_path.read_text(encoding="utf-8")
            print(f"ðŸŽ­ íŽ˜ë¥´ì†Œë‚˜ ë¡œë“œë¨: {persona_path.name}")
        except Exception:
            pass
    else:
        system_instruction = "ë„ˆëŠ” ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. ì‹¤ì‹œê°„ìœ¼ë¡œ ëŒ€í™”í•´."

    return {
        "response_modalities": ["AUDIO"],
        "speech_config": {
            "voice_config": {
                "prebuilt_voice_config": {
                    "voice_name": "Aoede"
                }
            }
        },
        "system_instruction": system_instruction
    }

# ==========================================
# [í´ëž˜ìŠ¤] DB ë¡œê·¸ ì €ìž¥ (SQLite)
# ==========================================
class DatabaseLogger:
    def __init__(self, db_path="chat_history.db"):
        self.db_path = db_path
        self.buffer = []
        self.session_id = None
        self._init_db()
        self._start_session()

    def _init_db(self):
        """DB í…Œì´ë¸” ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            # ì„¸ì…˜ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS sessions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    start_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    model_id TEXT
                )
            ''')
            # ë©”ì‹œì§€ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS messages (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id INTEGER,
                    sender TEXT,
                    content TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (session_id) REFERENCES sessions (id)
                )
            ''')
            conn.commit()

    def _start_session(self):
        """ìƒˆë¡œìš´ ëŒ€í™” ì„¸ì…˜ ì‹œìž‘"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('INSERT INTO sessions (model_id) VALUES (?)', (MODEL_ID,))
            self.session_id = cursor.lastrowid
            conn.commit()
        print(f"ðŸ’¾ DB ì„¸ì…˜ ì‹œìž‘ë¨: ID {self.session_id}")

    def append_text(self, text):
        self.buffer.append(text)

    def log_user_message(self, text):
        """ì‚¬ìš©ìž ë©”ì‹œì§€ ì €ìž¥"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO messages (session_id, sender, content) 
                    VALUES (?, ?, ?)
                ''', (self.session_id, 'user', text))
                conn.commit()
        except Exception as e:
            print(f"\nâš ï¸ DB ì €ìž¥ ì‹¤íŒ¨ (User): {e}")

    def flush_model_turn(self):
        """ëª¨ë¸ ì‘ë‹µ ì €ìž¥"""
        if not self.buffer: return
        
        full_text = "".join(self.buffer)
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO messages (session_id, sender, content) 
                    VALUES (?, ?, ?)
                ''', (self.session_id, 'gemini', full_text))
                conn.commit()
        except Exception as e:
            print(f"\nâš ï¸ DB ì €ìž¥ ì‹¤íŒ¨ (Gemini): {e}")
            
        self.buffer = []

# ==========================================
# [í´ëž˜ìŠ¤] STT ì²˜ë¦¬ê¸° (ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ)
# ==========================================
class SpeechTranscriber:
    def __init__(self, logger, shared_state=None):
        self.logger = logger
        self.shared_state = shared_state
        self.audio_queue = queue.Queue()
        self.running = True
        self.recognizer = sr.Recognizer()
        
        # STT ì„¤ì •
        self.energy_threshold = 1000  # ìŒì„± ê°ì§€ ìž„ê³„ê°’ (ì¡°ì ˆ í•„ìš”)
        self.pause_threshold = 0.8    # ë§ ëŠê¹€ ê°„ì£¼ ì‹œê°„ (ì´ˆ)
        self.sample_rate = 16000
        self.sample_width = 2         # 16-bit = 2 bytes

        self.thread = threading.Thread(target=self._process_loop, daemon=True)
        self.thread.start()
    
    def add_audio(self, data):
        if self.running:
            self.audio_queue.put(data)
            
    def stop(self):
        self.running = False
        self.thread.join(timeout=1.0)

    def _process_loop(self):
        print("ðŸ‘‚ STT ë¦¬ìŠ¤ë„ˆ ì‹œìž‘ (í•œêµ­ì–´)")
        
        audio_buffer = bytearray()
        silence_frames = 0
        has_voice = False
        
        # 1 í”„ë ˆìž„(ì²­í¬) ë‹¹ ì‹œê°„ ê³„ì‚°
        # CHUNK_SIZE(512) / RATE(16000) = 0.032ì´ˆ
        chunk_duration = 512 / 16000
        pause_frame_count = int(self.pause_threshold / chunk_duration)
        
        while self.running:
            try:
                # íì—ì„œ ì˜¤ë””ì˜¤ ì²­í¬ ê°€ì ¸ì˜¤ê¸° (íƒ€ìž„ì•„ì›ƒ 1ì´ˆ)
                data = self.audio_queue.get(timeout=1.0)
                
                # ì—ë„ˆì§€(ì†Œë¦¬ í¬ê¸°) ê³„ì‚°
                rms = audioop.rms(data, self.sample_width)
                
                if rms > self.energy_threshold:
                    has_voice = True
                    silence_frames = 0
                else:
                    if has_voice:
                        silence_frames += 1
                
                # ë²„í¼ì— ë°ì´í„° ì¶”ê°€
                if has_voice:
                    audio_buffer.extend(data)
                
                # ë§ì´ ëë‚¬ë‹¤ê³  íŒë‹¨ë˜ë©´ (ì¼ì • ì‹œê°„ ì¹¨ë¬µ)
                if has_voice and silence_frames > pause_frame_count:
                    # ì¸ì‹ ìˆ˜í–‰
                    self._recognize(audio_buffer)
                    
                    # ì´ˆê¸°í™”
                    audio_buffer = bytearray()
                    silence_frames = 0
                    has_voice = False
                    
                # ë²„í¼ê°€ ë„ˆë¬´ ì»¤ì§€ë©´ (ì˜ˆ: 15ì´ˆ ì´ìƒ) ê°•ì œ ì¸ì‹ (ë©”ëª¨ë¦¬ ë³´í˜¸)
                if len(audio_buffer) > 16000 * 2 * 15:
                    self._recognize(audio_buffer)
                    audio_buffer = bytearray()
                    silence_frames = 0
                    has_voice = False

            except queue.Empty:
                continue
            except Exception as e:
                print(f"STT ë£¨í”„ ì˜¤ë¥˜: {e}")
                
    def _recognize(self, audio_data):
        if len(audio_data) < 16000 * 2 * 0.5: # 0.5ì´ˆ ë¯¸ë§Œì€ ë¬´ì‹œ
            return
            
        try:
            # Raw PCM ë°ì´í„°ë¥¼ AudioData ê°ì²´ë¡œ ë³€í™˜
            audio_source = sr.AudioData(bytes(audio_data), self.sample_rate, self.sample_width)
            
            # Google Web Speech API í˜¸ì¶œ (ë™ê¸°)
            text = self.recognizer.recognize_google(audio_source, language="ko-KR")
            if text.strip():
                print(f"\n[ðŸ—£ï¸ User]: {text}")
                self.logger.log_user_message(text)
                
                # shared_state ì ‘ê·¼ì´ ì–´ë ¤ìš°ë¯€ë¡œ ë¡œê±°ë¥¼ í†µí•´ ìš°íšŒí•˜ê±°ë‚˜ ì „ì—­ ë³€ìˆ˜ ê³ ë ¤
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨ížˆ ì „ì—­ shared_stateê°€ ì—†ìœ¼ë¯€ë¡œ ìƒëžµí•˜ê±°ë‚˜ 
                # SpeechTranscriberì— shared_state ì°¸ì¡°ë¥¼ ë„˜ê²¨ì£¼ëŠ” ê²ƒì´ ì¢‹ìŒ
                if hasattr(self, 'shared_state') and self.shared_state:
                     self.shared_state["display_text"] = "..."
                
        except sr.UnknownValueError:
            # ì¸ì‹ ì‹¤íŒ¨ (ìž¡ìŒ ë“±) - ì¡°ìš©ížˆ ë„˜ì–´ê°
            pass
        except sr.RequestError as e:
            print(f"STT API ì˜¤ë¥˜: {e}")
        except Exception as e:
            print(f"STT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ==========================================
# [ë©”ì¸] ì‹¤í–‰ ë£¨í”„
# ==========================================

async def main():
    try:
        client = genai.Client(api_key=API_KEY)
        config = get_config()
        p = pyaudio.PyAudio()
        
        input_stream = None
        output_stream = None

        try:
            output_stream = p.open(format=AUDIO_FORMAT, channels=CHANNELS, rate=OUTPUT_RATE, output=True)
            input_stream = p.open(format=AUDIO_FORMAT, channels=CHANNELS, rate=INPUT_RATE, input=True, 
                                  input_device_index=MIC_DEVICE_INDEX, frames_per_buffer=CHUNK_SIZE)
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            return

        cap = cv2.VideoCapture(0)
        # ë‚´ í™”ë©´ìš© í•´ìƒë„ (ê³ í•´ìƒë„)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)

        if not cap.isOpened():
            print("âŒ ì›¹ìº ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"\nðŸš€ ëª¨ë¸({MODEL_ID}) ì—°ê²° ì¤‘...")


        # ê³µìœ  ë°ì´í„° ì»¨í…Œì´ë„ˆ (ë¯¸ë¦¬ ì •ì˜í•˜ì—¬ STTì— ì „ë‹¬)
        shared_state = {
            "latest_frame": None, 
            "running": True,
            "display_text": "ì•ˆë…•í•˜ì„¸ìš”!" 
        }

        logger = DatabaseLogger()
        stt_transcriber = SpeechTranscriber(logger, shared_state)

        try:
            async with client.aio.live.connect(model=MODEL_ID, config=config) as session:
                print("âœ… ì—°ê²° ì„±ê³µ! ëŒ€í™”ë¥¼ ì‹œìž‘í•˜ì„¸ìš”. (ì¢…ë£Œ: Ctrl+C ë˜ëŠ” í™”ë©´ì—ì„œ 'q')")
                
                # -------------------------------------------------------
                # [Task 1] ë¹„ë””ì˜¤ ì²˜ë¦¬ (í™”ë©´ í‘œì‹œ + ì „ì†¡ ë¶„ë¦¬)
                # -------------------------------------------------------
                
                async def capture_and_display():
                    print("ðŸ“· ì¹´ë©”ë¼ ìº¡ì²˜ ì‹œìž‘")
                    while shared_state["running"]:
                        ret, frame = cap.read()
                        if not ret: 
                            print("âŒ ì¹´ë©”ë¼ í”„ë ˆìž„ ì½ê¸° ì‹¤íŒ¨")
                            break

                        shared_state["latest_frame"] = frame.copy()

                        cv2.imshow('Gemini Live Vision', frame)
                        if cv2.waitKey(1) & 0xFF == ord('q'):
                            shared_state["running"] = False
                            break
                        
                        # í™”ë©´ ê°±ì‹  (ì•½ 30 FPS)
                        await asyncio.sleep(0.03)
                async def send_video_frames():
                    print("ðŸ“¡ ë¹„ë””ì˜¤ ì „ì†¡ ë°ëª¬ ì‹œìž‘")
                    while shared_state["running"]:
                        if shared_state["latest_frame"] is not None:
                            frame = shared_state["latest_frame"]
                            
                            # ì „ì†¡ ê·œê²© 640x480
                            frame_resized = cv2.resize(frame, (640, 480))
                            _, buffer = cv2.imencode('.jpg', frame_resized, [int(cv2.IMWRITE_JPEG_QUALITY), 50])
                            
                            try:
                                await session.send_realtime_input(
                                    video=types.Blob(
                                        data=buffer.tobytes(), 
                                        mime_type="image/jpeg"
                                    )
                                )
                            except TypeError:
                                await session.send_realtime_input(
                                    data=buffer.tobytes(), 
                                    mime_type="image/jpeg"
                                )
                            except Exception as e:
                                print(f"ë¹„ë””ì˜¤ ì „ì†¡ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
                        
                        # ì „ì†¡ ì£¼ê¸° (0.4ì´ˆ = 2.5 FPS)
                        await asyncio.sleep(0.4)
                
                # -------------------------------------------------------
                # [Task 2] ì˜¤ë””ì˜¤ ìž…ë ¥
                # -------------------------------------------------------
                async def send_audio_stream():
                    print("ðŸŽ™ï¸ ë§ˆì´í¬ ì „ì†¡ ì‹œìž‘")
                    while True:
                        try:
                            data = await asyncio.to_thread(input_stream.read, CHUNK_SIZE, exception_on_overflow=False)
                            
                            # STT ì²˜ë¦¬ë¥¼ ìœ„í•´ ë°ì´í„° ë³µì‚¬ë³¸ ì „ë‹¬
                            stt_transcriber.add_audio(data)
                            
                            # [ìˆ˜ì •] ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ audio=types.Blob(...) í˜•íƒœë¡œ ì „ì†¡
                            await session.send_realtime_input(
                                audio=types.Blob(
                                    data=data, 
                                    mime_type="audio/pcm;rate=16000"
                                )
                            )
                        except Exception as e:
                            print(f"ì˜¤ë””ì˜¤ ì „ì†¡ ì˜¤ë¥˜: {e}")
                            break

                # -------------------------------------------------------
                # [Task 3] ì‘ë‹µ ìˆ˜ì‹ 
                # -------------------------------------------------------
                async def receive_response():
                    while True:
                        try:
                            async for response in session.receive():
                                if response.server_content:
                                    model_turn = response.server_content.model_turn
                                    if model_turn:
                                        for part in model_turn.parts:
                                            if part.inline_data:
                                                output_stream.write(part.inline_data.data)
                                            if part.text:
                                                print(part.text, end="", flush=True)
                                                logger.append_text(part.text)

                                    # í„´ì´ ëë‚¬ëŠ”ì§€ í™•ì¸ (API ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìžˆìŒ)
                                    # turn_completeê°€ ëª…ì‹œì ìœ¼ë¡œ ì˜¤ë©´ ì €ìž¥
                                    if getattr(response.server_content, "turn_complete", False):
                                        logger.flush_model_turn()
                                        
                        except Exception as e:
                            print(f"ìˆ˜ì‹  ì˜¤ë¥˜: {e}")
                            break

                video_display_task = asyncio.create_task(capture_and_display())
                video_sender_task = asyncio.create_task(send_video_frames())
                audio_task = asyncio.create_task(send_audio_stream())
                recv_task = asyncio.create_task(receive_response())

                try:
                    # ì¹´ë©”ë¼ ì°½ì´ ë‹«íž ë•Œê¹Œì§€ ëŒ€ê¸°
                    await video_display_task
                except asyncio.CancelledError:
                    pass
                finally:
                    video_sender_task.cancel()
                    audio_task.cancel()
                    recv_task.cancel()

        except Exception as e:
            print(f"\nâŒ ì„¸ì…˜ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
        finally:
            stt_transcriber.stop()
            print("\nðŸ‘‹ ìƒë‹´ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

            if cap.isOpened(): cap.release()
            if input_stream: input_stream.stop_stream(); input_stream.close()
            if output_stream: output_stream.stop_stream(); output_stream.close()
            if p: p.terminate()
            cv2.destroyAllWindows()

    except Exception as e:
        print(f"\nâŒ ë©”ì¸ ì˜¤ë¥˜: {e}")
        input("ì—”í„°ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œí•©ë‹ˆë‹¤...")

if __name__ == "__main__":
    asyncio.run(main())